<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jialiang Pei</title>
    <link>https://hellosilicat.github.io/</link>
    <description>Recent content on Jialiang Pei</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 29 Feb 2020 18:50:56 +0800</lastBuildDate>
    
	<atom:link href="https://hellosilicat.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Test</title>
      <link>https://hellosilicat.github.io/life/test/</link>
      <pubDate>Sat, 29 Feb 2020 18:50:56 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/life/test/</guid>
      <description>分页测试。</description>
    </item>
    
    <item>
      <title>Create a New Customed Page in Hugo</title>
      <link>https://hellosilicat.github.io/posts/hugo-new-page/</link>
      <pubDate>Sat, 29 Feb 2020 18:40:44 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/hugo-new-page/</guid>
      <description> 之前的博客改造计划尚未结束，进行到文章分类步骤。Hugo官方提供了category/tag功能，即posts页面展示所有文章，category/tag页面展示分类/标签链接，但是我想要不同分类的文章独立在不同页面。在官方文档挣扎许久，最终针对black&amp;amp;white主题摸索出如下方案。（只创建了一个Life新页面，Sci&amp;amp;Tech采用默认页面）

Step1. hugo的content文件夹存放markdown文档，其目录结构对应域名结构，例如 content/xxx/yyy.md 文章对应 root:/xxx/yyy 页面，因此为了创建Life(自定义)页面，首先创建 content/life 文件夹，在该文件夹内创建_index.md(文件名不能错)文件，空的即可；
Step2. 之后创建 themes/xxx/layout/section 文件夹，在该文件夹里编写life.html，此处文件名需要和 content/life/ 对应，该文件内容可复制 *themes/xxx/layout/index.html*；
Step3. 为每一篇markdown文章添加type=&amp;quot;xxx&amp;quot;参数，例如Sci&amp;amp;Tech类文章是tech，而Life类文章是life，之后不同类的文章要指定type并且放到对应的content文件夹下；
Step4. 修改 themes/xxx/layout/index.html 和 themes/xxx/layout/section/life.html 模板参数，即 {{range where .Site.RegularPages &amp;quot;Params.type&amp;quot; &amp;quot;==&amp;quot; &amp;quot;ccc&amp;quot;}} ccc部分分别填tech和life。

这是针对Black&amp;amp;White主题的，具体步骤不适合其他主题，关键部分(content文件夹、layout/section/文件夹、_index.xxx文件、type参数与template用法)应该比较通用。
参考资料：  Hugo Variable Hugo Content Section  </description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://hellosilicat.github.io/about/</link>
      <pubDate>Wed, 01 Jan 2020 12:05:33 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/about/</guid>
      <description>Hi. I&amp;rsquo;m a Senior student at School of Computer Science in Beijing University of Posts and Telecommunications, majoring in Computer Science and Technology.
I write the blog for two reasons: collect the past(Life) and make convenience for future(Sci &amp;amp; Tech). If the third reason exists, I hope it&amp;rsquo;s for sharing something useful with you :)
I maintain a github account and you can contact me through email peijl@bupt.edu.cn. 
My experiences are as follow.</description>
    </item>
    
    <item>
      <title>Additive Model and Adaboost</title>
      <link>https://hellosilicat.github.io/posts/adaboost/</link>
      <pubDate>Sat, 28 Dec 2019 15:26:46 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/adaboost/</guid>
      <description>所谓提升方法(boosting)，其初衷旨在将多个弱学习器提升为强学习器，普遍做法是改变训练数据分布学习多个弱学习器并采取一定策略使用它们，因此了解某个boosting方法，我们需要明白它：
 迭代中如何改变训练数据分布？ 最终如何将弱学习器组合成强学习器？  本文首先引入boosting的贪心学习策略（加法模型与前向分步算法），之后推导式地介绍经典方法Adaboost。
0. 加法模型 将弱学习器提升为强学习器的一种做法是基函数的线性组合，即加法模型： $$f(x)=\sum \limits _{m=1}^{M} \beta _mb(x;\gamma _m)$$ 其中，$M$为基函数（弱学习器）个数，$b(x;\gamma _m)$为基函数，$\gamma _m$为基函数的参数，$\beta _m$为基函数的系数。
若给定损失函数$L(y,f(x))$，则boosting/加法模型的优化目标为： $$\min \limits _{\beta,\gamma} \sum \limits _{i=1}^N L \left( y _i, \sum \limits _{m=1}^{M} \beta _mb(x _i;\gamma _m)\right)$$
1. 前向分步算法 求解加法模型的全局最优解是一个复杂的优化问题，在计算上难度大，因此前向分步算法试图使用贪心策略，迭代式地计算每一个基函数，每一次迭代都去获取局部最优解，描述如下。
设前$m-1$次计算的加法模型为： $$f _{m-1}(x)=\sum \limits _{i=1}^{m-1} \beta _ib(x;\gamma _i)$$
则第$m$次优化目标为： $$(\beta _m, \gamma _m)=arg \min \limits _{\beta, \gamma} \sum \limits _{i=1}^N L \left( y _i, f _{m-1}(x _i) + \beta b(x _i;\gamma) \right)$$</description>
    </item>
    
    <item>
      <title>Single Node HBase: Build with Aliyun ECS</title>
      <link>https://hellosilicat.github.io/posts/hbase/</link>
      <pubDate>Mon, 04 Nov 2019 12:50:00 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/hbase/</guid>
      <description>0. Tips  Single Node means that I only have one Aliyun ECS.(1vCPU/2 GB/CentOS7) In hosts file, use private IP. You can get it from Aliyun Console The version of JDK, Hadoop and HBase must be compatible. (Google it!) If some process are not started, read the logs and find the errors(xxx/logs/xxx.log) If some paths are not existed, mkdir them Configure with private IP but visit from public IP  1.</description>
    </item>
    
    <item>
      <title>K-Means and DBSCAN</title>
      <link>https://hellosilicat.github.io/posts/cluster/</link>
      <pubDate>Thu, 31 Oct 2019 16:56:14 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/cluster/</guid>
      <description>1. K-Means Algorithm 给定样本集$D={x_1,x_2,&amp;hellip;,x_n}$，k-means算法针对聚类所得簇划分$C={C_1,C_2,&amp;hellip;,C_k}$最小化平方误差（$\mu _i = \frac{1}{C _i}\sum _{x\in C _i}x$）: $$E=\sum _{i=1}^k \sum _{x\in C _i}||x-\mu _i|| _2^2$$
然而，直接求解是NP-hard问题，故需要贪心迭代求解：
2. K-Means K值确定 轮廓系数(Sihouette Coefficient)：令凝聚度$a(i)$表示第$i$个样本与其同簇内所有其他元素的平均距离，分离度$b(i)$表示第i个样本与其他簇的最小平均距离，则轮廓系数可定义为： $$S = \frac{1}{n} \sum _{i=1} ^ n \frac{b(i)-a(i)}{max(a(i),b(i))}$$
如何确定K值？ 枚举一些k，分别计算轮廓系数，选择最大的。
另一种方法是肘部法：绘制代价函数与k的折线图，寻找折点。
3. K-Means 初始簇中心确定 K-Means++：初始的聚类中心相互距离尽可能远
 在样本集合中选择一个样本作为第一个聚类中心 对于其他样本x，计算它与最近的簇中心的距离$D(x)$ 选择一个新的数据点作为新的聚类中心，选择的原则是：$D(x)$较大的点，概率较大 重复2、3，直到选取了$k$个簇中心 继续K-Means  4. DBSCAN DBSCAN(Density-Based Spatial Clustering of Applications with Noise)基于一组“领域”参数$(e, MinPts)$来刻画样本分本的紧密程度，首先给出以下概念：
 $e$-邻域：对$x_j \in D$，其$e$-邻域包含样本集D中与$x_j$的距离不大于$e$的样本，即$$N _e (x _j) = {x _i \in D | dist(x _i, x_j) \le e}.</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://hellosilicat.github.io/posts/svm/</link>
      <pubDate>Tue, 15 Oct 2019 11:14:34 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/svm/</guid>
      <description>支持向量机(Support Vector Machine)既可以处理分类任务(SVC)，也可以处理回归任务(SVR)；既可以处理线性可分数据集(硬间隔SVM)，也能处理非线性可分数据集(软间隔SVM、核方法)。
1. 线性SVM 对于二分类线性可分数据集（正类表示为+1，负类表示为-1），设直线为$wx+b=0$能够分离不同类的样本点($wx+b&amp;gt;0$为正类，反之负类)，定义与直线距离最近的点$x _\alpha(\alpha=1,2,&amp;hellip;)$为支持向量，注意$y _i(wx _i+b)$的符号可以表示分类正确性(正代表分类正确，负表示分类错误)，则线性SVM的优化目标为直线到支持向量的距离最大，即： $$\max \limits _{w,b} \frac{y _\alpha(wx _\alpha+b)}{||w||}$$ $$s.t. \frac{y _i(wx _i +b)}{||w||} \ge \frac{y _\alpha (wx _\alpha +b)}{||w||}$$
因为同比例改变$w,b$不影响直线位置/形状，所以若$y _\alpha(wx _ \alpha+b)=\lambda$，则$(\frac{1}{\lambda * y _ \alpha}w, \frac{1}{\lambda * y _\alpha}b)$也是解，因此不妨设$y _\alpha (w x _\alpha +b)=1$，这样优化目标为(不等式两边同乘$|w|$)： $$\max \limits _{w,b} \frac{1}{||w||}$$ $$s.t. y _i(wx _i +b) \ge 1$$
为进行凸优化，$max \frac{1}{||w||}$可写成$min \frac{1}{2}||w||^2$，这样线性SVM最终的优化目标为： $$\min \limits _{w,b} \frac{1}{2}||w||^2$$ $$s.t. y _i(wx _i +b) \ge 1$$
拉格朗日乘子法对应的拉格朗日函数为： $$L(w,b,\alpha) = \frac{1}{2} ||w||^2 + \sum _{i=1}^n \alpha _i (1 - y _i (wx _i +b))$$</description>
    </item>
    
    <item>
      <title>Decision Tree and Random Forest</title>
      <link>https://hellosilicat.github.io/posts/dt-rf/</link>
      <pubDate>Mon, 14 Oct 2019 14:04:29 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/dt-rf/</guid>
      <description> 1. 基础 信息熵(information entropy)： 度量样本集合纯度，越小越纯；若当前样本集合$D$中第$i$类样本所占的比例为$p_i(i=1,2,&amp;hellip;,K)$，则信息熵定义为： $$Ent(D) = -\sum _{i=1}^K p_i logp_i$$
属性信息熵：度量样本集合按照特定属性分割后的信息熵；若样本集合在属性取$v$值时的子集为$D^v(v=1,2,&amp;hellip;,V)$，则属性信息熵定义为： $$\sum _{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)$$
信息增益：度量样本集合根据特定属性$a$分割后信息熵降低水平，偏好类别数多的属性，信息增益越大，分割后纯度越大，定义为： $$Gain(D,a)=Ent(D) - \sum _{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)$$
信息增益率：度量样本集合根据特定属性$a$分割后信息熵降低程度，偏好类别数较少的属性，信息增益率越大，分割后纯度越大，定义为： $$Gain-ratio(D,a) = \frac{Gain(D,a)}{-\sum _{v=1}^V \frac{|D^v|}{|D|} log\frac{|D^v|}{|D|} }$$
基尼系数：度量样本集合纯度，越小越纯(直观上代表从样本集合中抽出两个样本，它们类别不一致的概率)，定义为： $$Gini(D) = 1 - \sum _{i=1}^K p_i^2$$
属性基尼系数：度量样本集合按照特定属性$a$分割后纯度，越小越纯，定义为： $$Gini-index(D,a) = \sum _{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)$$
2. 决策树 ID3：分类树，采取信息增益率作为最优化分标准； C4.5：分类树，二分法离散化连续值，划分时，先选择信息增益高于平均水平的特征，再选择信息增益率最大的； CART：分类树/回归树，作为分类树时以属性基尼系数作为最优划分标准；作为回归树时参考回归树 基本算法： 剪枝：预剪枝在构建决策树时评估是否会带来泛化性的提升而选择是否分隔，训练速度快但是容易欠拟合；后剪枝从构建的决策树自叶节点向上，若将叶节点与父节点合并会提升泛化性则进行合并； 3. 随机森林 Bagging(Bootstrap Aggregating)：每次不放回采样m个样本进行训练，得到T个模型，分类则投票法，回归则平均法；Bootstrap方法每次约能涵盖63.2%的原始样本，剩余的36.8%可用作包外估计； 随机森林：基学习器是决策树，在Bagging的基础上，每次划分特征时，先随机选择k个特征，然后在这些特征中选择最优的，最终投票产生结果； 参考资料：  《机器学习》，周志华，清华大学出版社，2017 《统计学习方法》，李航，清华大学出版社，2012  </description>
    </item>
    
    <item>
      <title>Linear Regression and Logistic Regression</title>
      <link>https://hellosilicat.github.io/posts/2lr/</link>
      <pubDate>Sat, 12 Oct 2019 14:37:05 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/2lr/</guid>
      <description>1. 线性回归(单变量) 优化目标： 以均方误差为度量，损失函数定义为$E=\sum_{i=1}^n(y_i-wx_i-b)^2$，优化目标为：$\min\limits_{(w,b)} E $
求解： 求导得：$$\frac{\partial E}{\partial w}=2\big(w\sum_{i=1}^nx_i^2-\sum_{i=1}^n(y_i-b)x_i\big)$$ $$\frac{\partial E}{\partial b}=2\big(nb-\sum_{i=1}^n(y_i-wx_i)\big)$$
令偏导为零得(其中$\overline x = \frac{1}{n}x_i$)：$$w=\frac{\sum_{i=1}^ny_i(x_i-\overline x)}{\sum_{i=1}^nx_i^2-\frac{1}{n}(\sum_{i=1}^nx_i)^2}$$ $$b=\frac{1}{n}\sum_{i=1}^n(y_i-wx_i)$$
2. 线性回归(多变量) 优化目标： 令$w= (w_{original};b), X=(X_{original};1)$，损失函数定义为$E=(y-Xw)^T(y-Xw)$，优化目标为:$\min\limits_w E$
求解：仍然是凸函数，矩阵求导后置零计算参数。 求导得：$$\frac{\partial E}{\partial w} = 2X^T(Xw - y)$$
令导数为零($X^TX$为满秩矩阵或正定矩阵时，否则无解或多解)得：$$w=(X^TX)^{-1}X^Ty$$
3. 逻辑斯蒂回归 优化目标： 令$w=(w_{original};b),x_i=(x_i;1)$，在逻辑斯蒂回归中，$h(x_i)=\frac{1}{1+e^{-(w^Tx_i)}}$将线性回归的结果映射到$(0,1)$上(Logisitic Distribution)，其值可视作样本为正类$1$的概率(置信度)；若$p(y_i|x_i)$表示样本$x_i$被预测正确的概率，则可重写为：$p(y_i|x_i)=h(x_i)^{y_i}· (1-h(x_i))^{1-y_i}$。
假设样本之间独立，则样本集的似然函数为$\prod _{i=1}^nh(x_i)^{y_i}· (1-h(x_i))^{1-y_i}$，对数变换后得：$$L(w)=\sum _{i=1}^n y_i log\big(h(x_i)\big) +(1-y_i)log\big(1-h(x_i)\big)$$ 由极大似然法，优化目标为：$\min\limits_w (-L)$
求解：似然函数可通过凸优化的方式求解，这里采用梯度下降法。 不妨设$z = w^Tx_i$，对于参数$w_j$，计算偏导为： \begin{equation}\begin{split} \frac{\partial{(-L)}}{\partial w_j}&amp;amp;=-\sum _{i=1}^n x_i^j·\frac{e^{-z}}{(1+e^{-z})^2} · \big( \frac{y _i}{h(x_i)} - \frac{1-y _i}{1-h(x_i)} \big) \\
&amp;amp;=-\sum _{i=1}^n x_i^j·\frac{e^{-z}}{(1+e^{-z})^2} · \big( y _i(1+e^{-z}) - \frac{(1-y _i)(1+e^{-z})}{e^{-z}} \big)\\</description>
    </item>
    
    <item>
      <title>Confusion Matrix and Measurement</title>
      <link>https://hellosilicat.github.io/posts/confusion/</link>
      <pubDate>Thu, 10 Oct 2019 12:31:14 +0800</pubDate>
      
      <guid>https://hellosilicat.github.io/posts/confusion/</guid>
      <description> 在有监督分类学习中，混淆矩阵(confusion matrix)可用来描述模型在数据集上的预测状况，如针对$k$分类问题，混淆矩阵$C$为$k$阶方阵，其中$C_{i,j}(i,j\le k)$表示实际类别为$i$而模型预测类别为$j$的样本数；在混淆矩阵的基础上，我们可以定义各种度量用以评估模型性能，本文将介绍：准确率(accuracy)、精确率P(precision)、召回率R(recall)、F1值、P-R曲线与BEP(break-even point)、ROC曲线与AUC。
以二分类问题为例，类别包含正类$P$、负类$N$，其混淆矩阵如下图所示。其中：
 $TP$：实际为正类且预测为正类的样本数 $FN$：实际为正类而预测为负类的样本数 $FP$：实际为负类而预测为正类的样本数 $TN$：实际为负类而预测为负类的样本数  准确率Accuracy指在所有样本中模型预测正确的比例，即：$$Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$$
在类别不均衡问题中，准确率不能很好的度量性能，例如在99个正类和1个负类的数据集中，即使盲猜正类也能获得很高的准确率，然而这样的模型不具有泛化性，因此需要同时考虑精确率和召回率。
精确率P指在预测为正类的样本中预测正确的比例，即：$$Precision=\frac{TP}{TP+FP}$$
召回率R指在实际为正类的样本中预测正确的比例，即：$$Recall=\frac{TP}{TP+FN}$$
如何理解精确率和召回率？西瓜书中有一个例子：你去买西瓜，精确率关注你挑的瓜中有多少比例是好瓜，而召回率更关注有多少比例的好瓜被你挑了出来。
精确率与召回率是负相关的，精确率高时往往召回率会降低，反之亦然。如何综合两者去考量一个模型？P-R曲线与BEP便是方法之一。
P-R曲线横轴是召回率，纵轴是精确率，BEP(break-even point)是指曲线上横纵坐标相等的点，可以认为是平衡点。
如何绘制P-R曲线？对于一个样本集合，根据模型结果或中间值对样本进行排序，靠前的样本有更大概率被分为正类；之后逐个样本作为阈值，比该样本靠前的预测为正类，其余为负类，每次计算Precision和Recall，这样得到一系列二维点，绘制即可得到该曲线，通常情况下曲线为离散状，随数据集规模增加而更平滑。
如下图，当模型A的P-R曲线包含模型B的P-R曲线时，可认为A的性能更佳，如Model2优于Model1，然而P-R曲线也会出现交叉，此时可根据模型的BEP来判断，如Model3的BEP大于Model2的BEP(处于右上方)，则认为Model3的性能优于Model2。
通过P-R曲线来综合考虑精确率和召回率难免比较麻烦，另一个相对简单但有效的方式是计算F1值，F1值越大，模型性能越好。
F1-Score是精确率P和召回率R的调和平均数，即：$$\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})$$
当存在多个混淆矩阵(多次训练或多个数据集等产生)时，F1值可分为macro-F1和micro-F1，两者区别在于粒度不同。
macro-F1是指先计算出每个混淆矩阵的P和R，得到平均值后再计算F1值；
micro-F1是指先计算每个混淆矩阵的TP/FN/FP，得到平均值后计算P和R，最终再计算F1值；
不同的业务场景对精确率和召回率的要求是不同的，例如推荐系统中更关注推荐的东西是用户喜欢的，即更关注精确率，此时可通过加权F1值来评估模型。
加权F1-Score通过调整F1计算公式P、R系数来改变二者的重要程度，即：$$F_\beta=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})$$
当$\beta=1$时退化成普通F1，当$\beta&amp;gt;1$时更关注召回率R，当$\beta&amp;lt;1$时更关注精确率P(可分别计算P、R的偏导系数，越大说明该因子影响越大)。
与P-R曲线类似的是ROC(Receiver Operating Characteristic)曲线，也是先对样本进行排序，按此顺序逐片段作为正例进行预测，每次计算两个值，然后绘制曲线，不过ROC曲线的横轴是假阳率FPR(False Positive Rate)，纵轴为真阳率TPR(True Positive Rate)，其定义为： $$TPR=\frac{TP}{TP+FN}$$ $$FPR=\frac{FP}{TN+FP}$$
现实任务中，ROC曲线是离散的，如下图，一种绘制方法为首先将样本按照预测结果或中间值进行排序，然后依次将每个样本划分为正例，设正样本个数为$m^+$，负样本个数为$m-$，前一个标记点坐标为$(x,y)$，当前样本实际若为正例，则绘制$(x,y+\frac{1}{m^+})$，当前样本实际若为负例，则绘制$(x+\frac{1}{m^-},y)$，以此类推，ROC曲线必经过$(0,0)$点和$(1,1)$点，分别对应全部预测负类和全部预测正类的情况。
同样，当一个模型的ROC曲线包含包含另一个模型的ROC曲线时(所有点在左上方)，可认为该模型更优，然而面对ROC曲线的情况，需要引入AUC值来衡量模型好坏，AUC越大，说明模型性能更优。
AUC(Area Under ROC Curve)指ROC曲线下方的面积，将$x_{i}$ 按照大小顺序排列，AUC计算公式为(其中$m$指样本集大小)：$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})$$
参考资料：  《机器学习》，周志华，清华大学出版社，2017  </description>
    </item>
    
  </channel>
</rss>