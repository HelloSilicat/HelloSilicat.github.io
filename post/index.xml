<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Black &amp; Light</title>
    <link>https://www.example.com/post/</link>
    <description>Recent content in Posts on Black &amp; Light</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 10 Oct 2019 12:31:14 +0800</lastBuildDate>
    
	<atom:link href="https://www.example.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>混淆矩阵与性能度量</title>
      <link>https://www.example.com/post/confusion/</link>
      <pubDate>Thu, 10 Oct 2019 12:31:14 +0800</pubDate>
      
      <guid>https://www.example.com/post/confusion/</guid>
      <description>在有监督分类学习中，混淆矩阵(confusion matrix)可用来描述模型在数据集上的预测状况，如针对$k$分类问题，混淆矩阵$C$为$k$阶方阵，其中$C_{i,j}(i,j\le k)$表示实际类别为$i$而模型预测类别为$j$的样本数；直观如下图所示。
在混淆矩阵的基础上，我们可以定义各种度量用以评估模型性能，本文将介绍：准确率(accuracy)、精确率P(precision)、召回率R(recall)、F1值、P-R曲线与BEP(break-even point)、ROC曲线与AUC。
以二分类问题为例，类别包含正类$P$、负类$N$，其混淆矩阵如下图所示。其中：
 $TP$：实际为正类且预测为正类的样本数 $FN$：实际为正类而预测为负类的样本数 $FP$：实际为负类而预测为正类的样本数 $TN$：实际为负类而预测为负类的样本数  准确率Accuracy指在所有样本中模型预测正确的比例，即：$$Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$$
在类别不均衡问题中，准确率不能很好的度量性能，例如在99个正类和1个负类的数据集中，即使盲猜正类也能获得很高的准确率，然而这样的模型不具有泛化性，因此需要同时考虑精确率和召回率。
精确率P指在预测为正类的样本中预测正确的比例，即：$$Precision=\frac{TP}{TP+FP}$$
召回率R指在实际为正类的样本中预测正确的比例，即：$$Recall=\frac{TP}{TP+FN}$$
如何理解精确率和召回率？西瓜书中有一个例子：你去买西瓜，精确率关注你挑的瓜中有多少比例是好瓜，而召回率更关注有多少比例的好瓜被你挑了出来。
精确率与召回率是负相关的，精确率高时往往召回率会降低，反之亦然。如何综合两者去考量一个模型？P-R曲线与BEP便是方法之一。
P-R曲线横轴是召回率，纵轴是精确率，BEP(break-even point)是指曲线上横纵坐标相等的点，可以认为是平衡点。
如何绘制P-R曲线？对于一个样本集合，根据模型结果或中间值对样本进行排序，靠前的样本有更大概率被分为正类；之后逐个样本作为阈值，比该样本靠前的预测为正类，其余为负类，每次计算Precision和Recall，这样得到一系列二维点，绘制即可得到该曲线，通常情况下曲线为离散状，随数据集规模增加而更平滑。
如下图，当模型A的P-R曲线包含模型B的P-R曲线时，可认为A的性能更佳，如Model2优于Model1，然而P-R曲线也会出现交叉，此时可根据模型的BEP来判断，如Model3的BEP大于Model2的BEP(处于右上方)，则认为Model3的性能优于Model2。
 通过P-R曲线来综合考虑精确率和召回率难免比较麻烦，另一个相对简单但有效的方式是计算F1值，F1值越大，模型性能越好。
F1-Score是精确率P和召回率R的调和平均数，即：$$\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})$$
当存在多个混淆矩阵(多次训练或多个数据集等产生)时，F1值可分为macro-F1和micro-F1，两者区别在于粒度不同。
macro-F1是指先计算出每个混淆矩阵的P和R，得到平均值后再计算F1值；
micro-F1是指先计算每个混淆矩阵的TP/FN/FP，得到平均值后计算P和R，最终再计算F1值；
不同的业务场景对精确率和召回率的要求是不同的，例如推荐系统中更关注推荐的东西是用户喜欢的，即更关注精确率，此时可通过加权F1值来评估模型。
加权F1-Score通过调整F1计算公式P、R系数来改变二者的重要程度，即：$$F_\beta=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})$$
当$\beta=1$时退化成普通F1，当$\beta&amp;gt;1$时更关注召回率R，当$\beta&amp;lt;1$时更关注精确率P(可分别计算P、R的偏导系数，越大说明该因子影响越大)。
与P-R曲线类似的是ROC(Receiver Operating Characteristic)曲线，也是先对样本进行排序，按此顺序逐片段作为正例进行预测，每次计算两个值，然后绘制曲线，不过ROC曲线的横轴是假阳率FPR(False Positive Rate)，纵轴为真阳率TPR(True Positive Rate)，其定义为： $$TPR=\frac{TP}{TP+FN}$$ $$FPR=\frac{FP}{TN+FP}$$
现实任务中，ROC曲线是离散的，如下图，一种绘制方法为首先将样本按照预测结果或中间值进行排序，然后依次将每个样本划分为正例，设正样本个数为$m^+$，负样本个数为$m-$，前一个标记点坐标为$(x,y)$，当前样本实际若为正例，则绘制$(x,y+\frac{1}{m^+})$，当前样本实际若为负例，则绘制$(x+\frac{1}{m^-},y)$，以此类推，ROC曲线必经过$(0,0)$点和$(1,1)$点，分别对应全部预测负类和全部预测正类的情况。
 同样，当一个模型的ROC曲线包含包含另一个模型的ROC曲线时(所有点在左上方)，可认为该模型更优，然而面对ROC曲线的情况，需要引入AUC值来衡量模型好坏，AUC越大，说明模型性能更优。
AUC(Area Under ROC Curve)指ROC曲线下方的面积，将$x_{i}$ 按照大小顺序排列，AUC计算公式为(其中$m$指样本集大小)：$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})$$
参考资料：
 《机器学习》，周志华，清华大学出版社，2017  </description>
    </item>
    
  </channel>
</rss>