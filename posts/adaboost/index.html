<!DOCTYPE html>
<html lang="en">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  
  
  
  
  <link rel="prev" href="https://hellosilicat.github.io/posts/hbase/" />
  <link rel="next" href="https://hellosilicat.github.io/posts/hugo-new-page/" />
  <link rel="canonical" href="https://hellosilicat.github.io/posts/adaboost/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Additive Model and Adaboost | Jialiang Pei
       
  </title>
  <meta name="title" content="Additive Model and Adaboost | Jialiang Pei">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/hellosilicat.github.io\/"
    },
    "articleSection" : "posts",
    "name" : "Additive Model and Adaboost",
    "headline" : "Additive Model and Adaboost",
    "description" : "所谓提升方法(boosting)，其初衷旨在将多个弱学习器提升为强学习器，普遍做法是改变训练数据分布学习多个弱学习器并采取一定策略使用它们，因此了解某个boosting方法，我们需要明白它：\n 迭代中如何改变训练数据分布？ 最终如何将弱学习器组合成强学习器？  本文首先引入boosting的贪心学习策略（加法模型与前向分步算法），之后推导式地介绍经典方法Adaboost。\n0. 加法模型 将弱学习器提升为强学习器的一种做法是基函数的线性组合，即加法模型： $$f(x)=\\sum \\limits _{m=1}^{M} \\beta _mb(x;\\gamma _m)$$ 其中，$M$为基函数（弱学习器）个数，$b(x;\\gamma _m)$为基函数，$\\gamma _m$为基函数的参数，$\\beta _m$为基函数的系数。\n若给定损失函数$L(y,f(x))$，则boosting\/加法模型的优化目标为： $$\\min \\limits _{\\beta,\\gamma} \\sum \\limits _{i=1}^N L \\left( y _i, \\sum \\limits _{m=1}^{M} \\beta _mb(x _i;\\gamma _m)\\right)$$\n1. 前向分步算法 求解加法模型的全局最优解是一个复杂的优化问题，在计算上难度大，因此前向分步算法试图使用贪心策略，迭代式地计算每一个基函数，每一次迭代都去获取局部最优解，描述如下。\n设前$m-1$次计算的加法模型为： $$f _{m-1}(x)=\\sum \\limits _{i=1}^{m-1} \\beta _ib(x;\\gamma _i)$$\n则第$m$次优化目标为： $$(\\beta _m, \\gamma _m)=arg \\min \\limits _{\\beta, \\gamma} \\sum \\limits _{i=1}^N L \\left( y _i, f _{m-1}(x _i) \x2b \\beta b(x _i;\\gamma) \\right)$$",
    "inLanguage" : "en",
    "author" : "",
    "creator" : "",
    "publisher": "",
    "accountablePerson" : "",
    "copyrightHolder" : "",
    "copyrightYear" : "2019",
    "datePublished": "2019-12-28 15:26:46 \x2b0800 CST",
    "dateModified" : "2019-12-28 15:26:46 \x2b0800 CST",
    "url" : "https:\/\/hellosilicat.github.io\/posts\/adaboost\/",
    "wordCount" : "283",
    "keywords" : [  "Jialiang Pei"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://hellosilicat.github.io/">Jialiang Pei</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/life/" title="">Life</a>
                
                <a class="menu-item" href="/" title="">Sci &amp; Tech</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://hellosilicat.github.io/">Jialiang Pei</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/life/" title="">Life</a>
                
                <a class="menu-item" href="/" title="">Sci &amp; Tech</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Additive Model and Adaboost</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://hellosilicat.github.io/" rel="author"></a> with ♥ 
                <span class="post-time">
                on <time datetime=2019-12-28 itemprop="datePublished">December 28, 2019</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          

<p>所谓提升方法(boosting)，其初衷旨在将多个弱学习器提升为强学习器，普遍做法是改变训练数据分布学习多个弱学习器并采取一定策略使用它们，因此了解某个boosting方法，我们需要明白它：</p>

<ul>
<li>迭代中如何改变训练数据分布？</li>
<li>最终如何将弱学习器组合成强学习器？</li>
</ul>

<p>本文首先引入boosting的贪心学习策略（<strong>加法模型与前向分步算法</strong>），之后推导式地介绍经典方法<strong>Adaboost</strong>。</p>

<hr />

<h3 id="0-加法模型">0. 加法模型</h3>

<p>将弱学习器提升为强学习器的一种做法是基函数的线性组合，即加法模型：
$$f(x)=\sum \limits _{m=1}^{M} \beta _mb(x;\gamma _m)$$
其中，$M$为基函数（弱学习器）个数，$b(x;\gamma _m)$为基函数，$\gamma _m$为基函数的参数，$\beta _m$为基函数的系数。</p>

<p>若给定损失函数$L(y,f(x))$，则boosting/加法模型的优化目标为：
$$\min \limits _{\beta,\gamma} \sum \limits _{i=1}^N L \left( y _i, \sum \limits _{m=1}^{M} \beta _mb(x _i;\gamma _m)\right)$$</p>

<hr />

<h3 id="1-前向分步算法">1. 前向分步算法</h3>

<p>求解加法模型的全局最优解是一个复杂的优化问题，在计算上难度大，因此前向分步算法试图使用贪心策略，迭代式地计算每一个基函数，每一次迭代都去获取局部最优解，描述如下。</p>

<p>设前$m-1$次计算的加法模型为：
$$f _{m-1}(x)=\sum \limits _{i=1}^{m-1} \beta _ib(x;\gamma _i)$$</p>

<p>则第$m$次优化目标为：
$$(\beta _m, \gamma _m)=arg \min \limits _{\beta, \gamma} \sum \limits _{i=1}^N L \left(  y _i, f _{m-1}(x _i) + \beta b(x _i;\gamma) \right)$$</p>

<hr />

<h3 id="2-adaboost">2. Adaboost</h3>

<p>Adaboost作为经典提升方法，本质上是<strong>损失函数为指数函数的加法模型</strong>，以下以<u>二分类</u>为例，推导Adaboost的具体操作。</p>

<p>Adaboost在形式上也是基模型$G(x)$与系数$\alpha$构成的加法模型，即：
$$f(x)=\sum \limits _{m=1}^M \alpha _m G _m(x)$$
其损失函数为指数损失函数，即：
$$L(y,f(x)) = exp(-yf(x))$$</p>

<p>我们需要推导的便是：</p>

<ul>
<li>如何计算每一个$\alpha$？</li>
<li>如何改变数据分布计算每一个$G$？</li>
</ul>

<p>$$$$</p>

<p>假设经过$m-1$轮前向分步迭代已经求得$f _{m-1}(x)=\alpha _1 G _1(x) + &hellip; + \alpha _{m-1} G _{m-1}(x)$，由前向分步算法，可知：</p>

<p>\begin{equation}\begin{split}
(\alpha _m,G _m(x)) &amp;= arg \min \limits _{\alpha,G}   \sum \limits _{i=1}^N exp\left(  -y _i(f _{m-1}(x _i)+\alpha G(x _i)) \right) \\<br />
&amp;= arg \min \limits _{\alpha,G}   \sum \limits _{i=1}^N w _{m _i}exp\left( -y _i \alpha G(x _i) \right)
\end{split}\end{equation}
其中$w _{mi}=exp(-y _i f _{m-1}(x _i))$可以理解为第m个模型对于第i个样本的权值，用于修正训练数据分布；</p>

<p>对于上式，由于$\alpha$只是一个系数且$w _{mi}$与本次迭代无关，因此可以先计算最优的$G _m^{*}(x)$，由下式得到：
$$G _m^{*}(x)=arg \min \limits _G \sum \limits _{i=1}^N w _{mi}I(y _i \neq G(x _i))$$</p>

<p>之后求最优$\alpha ^{*}_m$，如下（<u>最后一个式子拆开分解成第二个式子更容易理解</u>）：</p>

<p>\begin{equation}\begin{split}
\sum \limits _{i=1}^N w _{mi} exp\left( -y _i\alpha G(x _i) \right) &amp;=  \sum \limits _{y _i = G _m(x _i)} w _{mi} e^{-\alpha} + \sum \limits _{y _i \neq G _m(x _i)} w _{mi} e ^{\alpha} \\<br />
&amp;= (e ^{\alpha} - e ^{-\alpha})\sum \limits _{i=1}^N w _{mi} I(y _i \neq G(x _i)) + e ^{-\alpha}\sum \limits _{i=1}^N w _{mi}
\end{split}\end{equation}</p>

<p>对$\alpha$求偏导并置零，将$G _m^{*}(x)$代入，可得：
$$\alpha _m^{*} = \frac{1}{2}log\frac{1-e _m}{e _m}$$
其中$e _m$是分类误差率：
$$e _m = \frac{\sum \limits _{i=1}^N w _{mi}I(y _i \neq G _m(x _i))}{\sum \limits _{i=1}^N w _{mi}} = \sum \limits _{i=1}^N I(y _i \neq G _m(x _i))$$</p>

<p>而样本分布修正因子$w _{m+1,i}$也可以通过下式获得：
$$w _{m+1,i} = exp(-y _i f _{m}(x _i)) = w _{mi} exp(-y _i \alpha _m^{*}G _m^{*}(x))$$</p>

<hr />

<h4 id="参考资料">参考资料：</h4>

<ol>
<li>《统计学习方法》，李航，清华大学出版社，2012</li>
</ol>

    </div>

    <div class="post-copyright">
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://hellosilicat.github.io/posts/adaboost/>https://hellosilicat.github.io/posts/adaboost/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://hellosilicat.github.io/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://hellosilicat.github.io/posts/hbase/" class="prev" rel="prev" title="Single Node HBase: Build with Aliyun ECS"><i class="iconfont icon-left"></i>&nbsp;Single Node HBase: Build with Aliyun ECS</a>
         
        
        <a href="https://hellosilicat.github.io/posts/hugo-new-page/" class="next" rel="next" title="Create a New Customed Page in Hugo">Create a New Customed Page in Hugo&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
