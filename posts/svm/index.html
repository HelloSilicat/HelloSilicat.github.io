<!DOCTYPE html>
<html lang="en">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  
  
  
  
  <link rel="prev" href="https://hellosilicat.github.io/posts/dt-rf/" />
  <link rel="next" href="https://hellosilicat.github.io/posts/cluster/" />
  <link rel="canonical" href="https://hellosilicat.github.io/posts/svm/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Support Vector Machine | Jialiang Pei
       
  </title>
  <meta name="title" content="Support Vector Machine | Jialiang Pei">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/hellosilicat.github.io\/"
    },
    "articleSection" : "posts",
    "name" : "Support Vector Machine",
    "headline" : "Support Vector Machine",
    "description" : "支持向量机(Support Vector Machine)既可以处理分类任务(SVC)，也可以处理回归任务(SVR)；既可以处理线性可分数据集(硬间隔SVM)，也能处理非线性可分数据集(软间隔SVM、核方法)。\n1. 线性SVM 对于二分类线性可分数据集（正类表示为\x2b1，负类表示为-1），设直线为$wx\x2bb=0$能够分离不同类的样本点($wx\x2bb\x26gt;0$为正类，反之负类)，定义与直线距离最近的点$x _\\alpha(\\alpha=1,2,\x26hellip;)$为支持向量，注意$y _i(wx _i\x2bb)$的符号可以表示分类正确性(正代表分类正确，负表示分类错误)，则线性SVM的优化目标为直线到支持向量的距离最大，即： $$\\max \\limits _{w,b} \\frac{y _\\alpha(wx _\\alpha\x2bb)}{||w||}$$ $$s.t. \\frac{y _i(wx _i \x2bb)}{||w||} \\ge \\frac{y _\\alpha (wx _\\alpha \x2bb)}{||w||}$$\n因为同比例改变$w,b$不影响直线位置\/形状，所以若$y _\\alpha(wx _ \\alpha\x2bb)=\\lambda$，则$(\\frac{1}{\\lambda * y _ \\alpha}w, \\frac{1}{\\lambda * y _\\alpha}b)$也是解，因此不妨设$y _\\alpha (w x _\\alpha \x2bb)=1$，这样优化目标为(不等式两边同乘$|w|$)： $$\\max \\limits _{w,b} \\frac{1}{||w||}$$ $$s.t. y _i(wx _i \x2bb) \\ge 1$$\n为进行凸优化，$max \\frac{1}{||w||}$可写成$min \\frac{1}{2}||w||^2$，这样线性SVM最终的优化目标为： $$\\min \\limits _{w,b} \\frac{1}{2}||w||^2$$ $$s.t. y _i(wx _i \x2bb) \\ge 1$$\n拉格朗日乘子法对应的拉格朗日函数为： $$L(w,b,\\alpha) = \\frac{1}{2} ||w||^2 \x2b \\sum _{i=1}^n \\alpha _i (1 - y _i (wx _i \x2bb))$$",
    "inLanguage" : "en",
    "author" : "",
    "creator" : "",
    "publisher": "",
    "accountablePerson" : "",
    "copyrightHolder" : "",
    "copyrightYear" : "2019",
    "datePublished": "2019-10-15 11:14:34 \x2b0800 CST",
    "dateModified" : "2019-10-15 11:14:34 \x2b0800 CST",
    "url" : "https:\/\/hellosilicat.github.io\/posts\/svm\/",
    "wordCount" : "728",
    "keywords" : [  "Jialiang Pei"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://hellosilicat.github.io/">Jialiang Pei</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/life/" title="">Life</a>
                
                <a class="menu-item" href="/" title="">Sci &amp; Tech</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://hellosilicat.github.io/">Jialiang Pei</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                <a class="menu-item" href="/life/" title="">Life</a>
                
                <a class="menu-item" href="/" title="">Sci &amp; Tech</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Support Vector Machine</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://hellosilicat.github.io/" rel="author"></a> with ♥ 
                <span class="post-time">
                on <time datetime=2019-10-15 itemprop="datePublished">October 15, 2019</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          

<p>支持向量机(Support Vector Machine)既可以处理分类任务(SVC)，也可以处理回归任务(SVR)；既可以处理线性可分数据集(硬间隔SVM)，也能处理非线性可分数据集(软间隔SVM、核方法)。</p>

<hr />

<h3 id="1-线性svm">1. 线性SVM</h3>

<p>对于二分类线性可分数据集（正类表示为+1，负类表示为-1），设直线为$wx+b=0$能够分离不同类的样本点($wx+b&gt;0$为正类，反之负类)，定义与直线距离最近的点$x _\alpha(\alpha=1,2,&hellip;)$为支持向量，注意$y _i(wx _i+b)$的符号可以表示分类正确性(正代表分类正确，负表示分类错误)，则线性SVM的优化目标为直线到支持向量的距离最大，即：
$$\max \limits _{w,b} \frac{y _\alpha(wx _\alpha+b)}{||w||}$$
$$s.t. \frac{y _i(wx _i +b)}{||w||} \ge \frac{y _\alpha (wx _\alpha +b)}{||w||}$$</p>

<p>因为同比例改变$w,b$不影响直线位置/形状，所以若$y _\alpha(wx _ \alpha+b)=\lambda$，则$(\frac{1}{\lambda * y _ \alpha}w, \frac{1}{\lambda * y _\alpha}b)$也是解，因此不妨设$y          _\alpha (w x _\alpha +b)=1$，这样优化目标为(不等式两边同乘$|w|$)：
$$\max \limits _{w,b} \frac{1}{||w||}$$
$$s.t. y _i(wx _i +b) \ge 1$$</p>

<p>为进行凸优化，$max \frac{1}{||w||}$可写成$min \frac{1}{2}||w||^2$，这样线性SVM最终的优化目标为：
$$\min \limits _{w,b} \frac{1}{2}||w||^2$$
$$s.t. y _i(wx _i +b) \ge 1$$</p>

<p><a href="https://www.cnblogs.com/sddai/p/5728195.html">拉格朗日乘子法</a>对应的拉格朗日函数为：
$$L(w,b,\alpha) = \frac{1}{2} ||w||^2 + \sum _{i=1}^n \alpha _i (1 - y _i (wx _i +b))$$</p>

<p>简单写一下我对<a href="https://www.cnblogs.com/lijiankou/p/3308742.html">拉格朗日对偶性</a>的理解：原优化目标为凸函数，故对应的拉格朗日函数需求最小极值点，分别对未知参数求偏导等价于$\min \limits _{w,b} \max \limits _{\alpha} L(w,b,\alpha)$，$min$容易理解，为什么需要$max$呢，一旦存在某个$\alpha _i g(x)&gt; \alpha _{max}(x)$（g(x)是约束函数，小于等于零），那么$\alpha _{max}$就可以替换成$\alpha _{i}$从而使整个式子更小，算是节省了对$\alpha$求导的代价吧，拉格朗日对偶性是指在满足KKT条件后，上述极小极大问题等价于极大极小问题，即$$\max \limits _{\alpha} \min \limits _{w,b}  L(w,b,\alpha)$$</p>

<p>先求$\min \limits _{w,b} L(w,b,\alpha)$令$L$关于$w,b$的偏导为零可得：
$$w = \sum _{i=1}^n \alpha _i y _i x _i$$
$$0 = \sum _{i=1}^n \alpha _i y_i$$</p>

<p>代入拉格朗日函数得到：
$$L(\alpha) = \sum _{i=1}^n \alpha _i - \frac{1}{2} \sum _{i=1}^n \sum _{j=1}^n \alpha _i \alpha _j y _i y _j (x _i ^T x_j)$$</p>

<p>因此等价的对偶优化目标为：
$$\max \limits _{\alpha} L(\alpha)$$
$$s.t. \sum _{i=1}^n \alpha _i y_i = 0, \alpha _i \ge 0 $$</p>

<p>同时满足KKT条件：
$$\alpha _i \ge 0, \\\ y _i(w x _i  + b) - 1 \ge 0, \\\ \alpha _i (y _i(w x _i + b) - 1) = 0.$$</p>

<p>最终：
$$w = \sum _{i=1}^n \alpha _i y _i x _i$$
$$b = \frac{1- y _j w^Tx _j}{y _j}，其中\alpha _j &gt; 0(根据KKT条件推导)$$</p>

<p>观察到，$w$只和$\alpha _i \ne 0$的样本点有关，而根据KKT条件，这些样本点满足$y _i(w x _i  + b) = 1$，即支持向量，所以SVM模型在预测时仅和支持向量有关，这样的稀疏性大大降低了时间开销。</p>

<hr />

<h3 id="2-软间隔svm">2. 软间隔SVM</h3>

<p>对于非线性可分数据集，软间隔SVM允许部分样本点被分错，但是希望这样的错误越少越好，为了达到这样的目标，可以采取损失函数或者引入松弛变量的方法。</p>

<p>对应不同的损失函数，软间隔SVM的优化目标为(C为超参数，调节“软”的程度)：</p>

<ul>
<li>hinge损失: $\min \limits _{w,b} \frac{1}{2}||w||^2 + C\sum _{i=1}^n max(0, 1 - y_i(wx_i+b))$</li>
<li>指数损失:   $\min \limits _{w,b} \frac{1}{2}||w||^2 + C\sum _{i=1}^n e^{-1 + y_i(wx_i+b))}$</li>
<li>对率损失: $\min \limits _{w,b} \frac{1}{2}||w||^2 + C\sum _{i=1}^n log(1+e^{-1 + y_i(wx_i+b)})$</li>
</ul>

<p>可以通过引入松弛变量来表达软间隔SVM，其优化式为(引入$\xi$作为松弛变量，表达每个样本被分错的程度)：
$$\min \limits_{w,b,\xi} \frac{1}{2}||w||^2 + C \sum_{i=1}^{n}\xi _i$$
$$s.t. y_i(w^Tx_i+b)\ge 1-\xi _i$$
$$\xi _i \ge 0, i=1,2,&hellip;,n$$</p>

<p>同样，其拉格朗日函数为：
$$L(w,b,\alpha,\xi _i,\mu) = \frac{1}{2}||w||^2 + C\sum _{i=1}^{n}\xi _i+\sum _{i=1}^b \alpha _i(1-\xi _i-y _i(w^Tx _i+b)) - \sum_{i=1}^n \mu _i\xi _i$$</p>

<p>同样，先视拉格朗日乘子$\alpha,\mu$为常数，对$w,b,\xi$求偏导得：
$$w=\sum _{i=1}^n \alpha _i y_i x_i$$
$$\sum _{i=1}^n \alpha _iy_i$$
$$C = \alpha _i + \mu _i$$</p>

<p>代入得对偶问题：
$$\max \limits_{\alpha} \sum _{i=1}^n \alpha - \frac{1}{2}\sum _{i=1}^n \sum _{j=1}^n \alpha _i \alpha _j y_iy_jx_i^Tx_j$$
$$s.t. \sum _{i=1}^n \alpha _i y _i = 0,$$
$$0\le\alpha _i\le C,i=1,2,&hellip;,n$$</p>

<p>其KKT条件为：
$$\alpha _i \ge 0, \\\ \mu _i \ge 0, \\\ y _i(w x _i  + b) - 1+ \xi _i \ge 0, \\\ \xi _i \ge 0, \\\ \alpha _i (y _i(w x _i + b) - 1 + \xi) = 0, \\\ \mu _i \xi _i = 0$$</p>

<hr />

<h3 id="3-核方法">3. 核方法</h3>

<p>另一个针对非线性可分数据集的SVM是引入核方法，可证明：如果原始空间是有限集，即属性数有限，那么一定存在一个高维特征空间使样本可分。</p>

<p>引入$\phi$表示原始向量映射后的特征向量，则硬间隔SVM的优化目标可改写为：
$$\min \limits _{w,b} \frac{1}{2}||w||^2,$$
$$s.t. y_i(w^T\phi(x) _i + b) \ge 1$$</p>

<p>其拉格朗日对偶问题为：
$$\max \limits _{\alpha} \sum _{i=1}^n \alpha _i -\frac{1}{2} \sum _{i=1}^n \sum _{j=1} ^n \alpha _i \alpha _j y _i y _j \phi(x _i)^T \phi(x _j)$$
$$s.t. \sum _{i=1} ^ n \alpha _i y _i =0,$$
$$\alpha _i \ge 0, i = 1,2,&hellip;,n$$</p>

<p>我们发现对于对偶问题，只需要求得原始空间向量内积在映射空间的内积即可，定义核函数$K(x_i,x_j)$表达$x_i,x_j$在映射空间的内积，则求解对偶问题后可得：
\begin{equation}\begin{split}
f(x) &amp;= w^T\phi (x) + b \\<br />
&amp;= \sum _{i=1} ^ n \alpha _i y_i \phi (x_i)^T\phi (x_j) + b\\<br />
&amp; = \sum _{i=1} ^ n \alpha _i y _i K(x,x_i) + b
\end{split}\end{equation}</p>

<p>常见的核函数：</p>

<ul>
<li>线性核： $K(x,y) = x^Ty$</li>
<li>多项式核：$K(x,y)=(x^Ty)^d$</li>
<li>高斯核：$K(x,y)=e^{-\frac{||x-y||^2}{2\sigma ^2}}$</li>
</ul>

<hr />

<h3 id="4-支持向量回归">4. 支持向量回归</h3>

<p>支持向量的出发点是，对于每一个样本点，允许其出现偏差，上限为$e$，偏差超过$e$才开始计入，形式化为($||z|| &lt; e$时,$l(z)=0$，否则$l(z) = ||z||-e$)：
$$\min \limits _{w,b} \frac{1}{2} ||w||^2 + C \sum _{i=1}^{n} l(f(x_i),y_i)$$</p>

<p>引入松弛变量，SVR的优化目标为：
$$\min \limits _{w,b,\xi _i, \hat \xi _i} = \frac{1}{2} ||w||^2 + C \sum _{i=1}^n (\xi _i + \hat \xi _i)$$
$$s.t.  f(x_i)-y_i \le e+ \xi _i,$$
$$y_i - f(x_i) &lt;= e + \hat \xi _i$$
$$\xi _i \ge 0, \hat \xi _i \ge 0$$</p>

<p>对应的拉格朗日函数为：
\begin{equation}\begin{split}
L&amp;(w,b,\alpha,\hat \alpha, \mu, \hat \mu, \xi, \hat \xi)   \\<br />
&amp;= \frac{1}{2} ||w||^2 + C \sum _{i=1} ^ n (\xi _i + \hat \xi _j) - \sum _{i=1}^n \mu _i \xi _i - \sum _{i=1}^n \hat \mu \hat \xi _i \\<br />
&amp; + \sum _{i=1}^n \alpha _i (f(x_i)-y_i-e-\xi _i) + \sum _{i=1} ^ n \hat \alpha _i(y _i  - f(x _i) - e - \hat \xi _i)
\end{split}\end{equation}</p>

<p>对$w,b,\xi,\hat \xi$求偏导得：
$$w=\sum _{i=1} ^ b (\hat \alpha _i - \alpha _i)x _i,$$
$$0=\sum _{i=1}^n(\hat \alpha _i - \alpha _i),$$
$$C=\alpha _i + \mu _i,$$
$$C=\hat \alpha _i + \hat \mu _i$$</p>

<p>代入得对偶问题为：
$$\max \limits _{\alpha _i, \hat \alpha _i} \sum _{i=1}^n y_i(\hat \alpha _i - \alpha _i) - e(\hat \alpha _i + \alpha _i) - \frac{1}{2}\sum _{i=1}^n \sum _{j=1}^n (\hat \alpha _i - \alpha _i)(\hat \alpha _j - \alpha _j)x_i^Tx_j $$
$$s.t. \sum _{i=1}^n (\hat \alpha _i - \alpha _i) = 0,$$
$$0 \le \alpha _i, \hat \alpha _i \le C$$</p>

<hr />

<h4 id="参考资料">参考资料：</h4>

<ol>
<li>《机器学习》，周志华，清华大学出版社，2017</li>
</ol>

    </div>

    <div class="post-copyright">
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://hellosilicat.github.io/posts/svm/>https://hellosilicat.github.io/posts/svm/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://hellosilicat.github.io/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://hellosilicat.github.io/posts/dt-rf/" class="prev" rel="prev" title="Decision Tree and Random Forest"><i class="iconfont icon-left"></i>&nbsp;Decision Tree and Random Forest</a>
         
        
        <a href="https://hellosilicat.github.io/posts/cluster/" class="next" rel="next" title="K-Means and DBSCAN">K-Means and DBSCAN&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
